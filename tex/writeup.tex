\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[margin = 1 in]{geometry}



\title{CS5220 Project 2: Shallow Water Equations}
\author{Nick Cebry, Jiahao Li, Max Ruth}
\date{October 5, 2020}

\begin{document}

\maketitle

\section{Introduction}
In this project, we worked with the a finite volume solver for the shallow
water equations. These PDEs model the movement of waves in scenarios where
the waves are long compared to the depth of the water. We were provided
with a moderately performant single threaded version of the code, and made
an effort to improve performance by parallelizing the algorithm. For our
implementation, we chose to use MPI as our parallelization paradigm. Our
algorithm divides the grid up into a series of subdomains, and assigns one
subdomain to each processor. Processors are responsible for calculating
the behavior of the water in their portion of the domain. They communicate
information about the cells on the edge of their domain with the
processors of neighboring domains so that information propagates properly
across domain boundaries.

\section{The Algorithm}
The general process:
\begin{enumerate}
	\item Used MPI
	\item Transfer Boundary information to left and right neighbors, the top and bottom (transferring the corners to top and bottom)
	\item Transfer boundary information every $\tau$ steps
	\item Transfer time step information every $\tau$ steps as well (This is what is being used in the model anyway)
\end{enumerate}
The model
\begin{enumerate}
	\item Four different contributions to batches of time steps
	\item Give predictions for how strong scaling, weak scaling, and optimal time batching look like.
\end{enumerate}

\section{Scaling and Profiling Results}
We performed both strong and weak scaling studies to analyze the
performance of our algorithm. For the strong scaling study, we used the
dam break scenario, and fixed the resolution of the grid cells at 1000
cells in both the X and Y dimensions. We then varied the number of
processors used to compute the simulation up until a fixed simulation
time. The size of the sub-domains varied with the number of processors in
order to keep the problem size constant. We compare the wall clock time
required to solve a fixed problem as the amount of compute resources
available varies. For the weak scaling study, we again used the dam break
scenario, but varied the resolution of the grid cells with the number of
processors used. The resolution of the grid was varied such that each sub-
domain was always 300 cells on each side. The simulation was run until a
fixed amount of time had been simulated. In order to account for the
different CFL conditions with different grid resolutions, we recorded the
total number of time steps processed, and divided the wall clock time by
this number to determine the average amount of time required to compute a
single tick of the simulation. We compare the wall clock time required to
compute a single simulation tick as both the size of the problem and the
computational resources available scale.



\section{Conclusion}
What we would add for next time:
\begin{enumerate}
	\item More careful cache performance
	\item Tuning number of ghost cells to block size
	\item Different sized domains/initial conditions
	\item Deal with the fact that not all processors are the same
	\item Think about the tradeoff between a conservative time step and communicating every step (how uneven are the time steps really?)
	\item Communication between processors at different nodes is different than that of the same nodes?
\end{enumerate}


\end{document}
