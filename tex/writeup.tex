\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[margin = 1 in]{geometry}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}

\title{CS5220 Project 2: Shallow Water Equations}
\author{Nick Cebry, Jiahao Li, Max Ruth}
\date{October 5, 2020}

\begin{document}

\maketitle

\section{Introduction}
In this project, we worked with the a finite volume solver for the shallow
water equations. These PDEs model the movement of waves in scenarios where
the waves are long compared to the depth of the water. We were provided
with a moderately performant single threaded version of the code, and made
an effort to improve performance by parallelizing the algorithm. For our
implementation, we chose to use MPI as our parallelization paradigm. Our
algorithm divides the grid up into a series of subdomains, and assigns one
subdomain to each processor. Processors are responsible for calculating
the behavior of the water in their portion of the domain. They communicate
information about the cells on the edge of their domain with the
processors of neighboring domains so that information propagates properly
across domain boundaries.

\section{The Algorithm}
\textbf{For the purposes of this algorithm, we are considering two consecutive calls to \texttt{central2d\_step} to be ``one time step,'' in order to alleviate any confusion with the shifting grids in Jiang and Tadmor.}

To address the problem of parallelizing the method of Jiang and Tadmor, we use a domain decomposition method like the one introduced for Conway's Game of Life. For the problem, we assume that the domain $\Omega = (0,L)^2$ is divided into square cells, with a resolution of $M$ cells in each direction. Because the problem is periodic, this amounts to $M$ ``points'' at which the height $h$, $x$-velocity $u$, and $y$-velocity $v$ are known. To parallelize the code, we divide the domain into an $N_x\times N_y$ grid of subdomains, where each subdomain is owned by a process. By this construction, we are using a total of $N_x N_y$ processors. 

As a part of this construction, each subdomain is responsible for knowing the value of $m_x \times m_y$ points \textit{at all times} where $m_x = M/N_x$ and $m_y = M/N_y$. However, due to the way information flows through a hyperbolic system of equations, we also require each processor to store a ``halo'' of ghost cells with width $m_g.$ Each processor is not tasked with knowing the value of the ghost cells, but rather retrieves the values of the ghost cells from its neighbors. The width of the halo is related to how many time steps each processor can perform independently before information must be exchanged -- called $m_t$ -- by the relation $m_g = 4 m_t$. Independent of the number of processors (assuming $M$ is fixed), the we will call the total number of time step blocks $N_t$, so that the total number of time steps is $m_t N_t$. 

We must also keep track of the maximum speed of information across all of the processors, so that each processor takes the same time step. In the code, we are currently sharing this information \textit{at every time step}, but this turns out not to be a large burden for the sizes of problems we are considering. 

In total, the algorithm can be summarized by Alg.~\ref{alg:PJT}. Inside of the main loop, we first share the ghost cells. This goes in the order of pass-left and receive-right; pass-right and receive-left; pass-up and receive-down; pass-down and receive-up. In the left/right passing of ghost cells, we pass a block of size $m_g m_y$, whereas for up/down we pass a block of size $m_g (m_x + 2m_g)$, as we need to communicate the corners of the halo along with the sides in this step. 
\begin{algorithm}
\caption{Parallel Jiang-Tadmor Main Loop}\label{alg:PJT}
\begin{algorithmic}
\For {$i \gets 1$ to $N_t$}
    \State Share ghost cells via 4 calls of \texttt{MPI\_Sendrecv}
    \For{$j \gets 1$ to $m_t$}
        \State Get local time step using \texttt{speed}
        \State Get global time step via \texttt{MPI\_Allreduce}
        \State Perform time step
    \EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

\subsection{Model Speed Up}



The general process:
\begin{enumerate}
	\item Used MPI
	\item Transfer Boundary information to left and right neighbors, the top and bottom (transferring the corners to top and bottom)
	\item Transfer boundary information every $\tau$ steps
	\item Transfer time step information every $\tau$ steps as well (This is what is being used in the model anyway)
\end{enumerate}
The model
\begin{enumerate}
	\item Four different contributions to batches of time steps
	\item Give predictions for how strong scaling, weak scaling, and optimal time batching look like.
\end{enumerate}

\section{Scaling and Profiling Results}
We performed both strong and weak scaling studies to analyze the
performance of our algorithm. For the strong scaling study, we used the
dam break scenario, and fixed the resolution of the grid cells at 1000
cells in both the X and Y dimensions. We then varied the number of
processors used to compute the simulation up until a fixed simulation
time. The size of the sub-domains varied with the number of processors in
order to keep the problem size constant. We compare the wall clock time
required to solve a fixed problem as the amount of compute resources
available varies. For the weak scaling study, we again used the dam break
scenario, but varied the resolution of the grid cells with the number of
processors used. The resolution of the grid was varied such that each sub-
domain was always 300 cells on each side. The simulation was run until a
fixed amount of time had been simulated. In order to account for the
different CFL conditions with different grid resolutions, we recorded the
total number of time steps processed, and divided the wall clock time by
this number to determine the average amount of time required to compute a
single tick of the simulation. We compare the wall clock time required to
compute a single simulation tick as both the size of the problem and the
computational resources available scale. Additionally, for both types of
scaling tests we analyzed the impact of varying the frequency of
communication between sub-domains. These tests explored the tradeoff
between frequency of suffering the penalty of communication and amount
of data communicated and duplicated computation performed.

\begin{table}
\resizebox{\textwidth}{!}{
\begin{tabular}{cccccccccc}
  \toprule
  sub-domain grid & nproc & \multicolumn{8}{c}{simulation ticks per communication} \\\cmidrule(r){3-10}
                  &                 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\\midrule
  1x1 &  1 & 138.95 & 140.02 & 136.78 & 130.53 & 119.90 & 130.60 & 160.26 & 171.99 \\\midrule
  2x2 &  4 &  48.23 &  48.17 &  48.62 &  29.87 &  47.93 &  51.60 &  30.35 &  51.15 \\\midrule
  3x3 &  9 &  30.36 &  30.65 &  32.47 &  34.00 &  30.08 &  35.37 &  32.00 &  39.20 \\\midrule
  4x4 & 16 &   9.50 &  10.78 &  12.11 &  13.88 &  14.88 &  17.04 &  16.48 &  20.62 \\\midrule
  5x5 & 25 &   3.76 &   3.72 &   3.89 &   4.41 &   4.63 &   6.23 &   6.52 &   8.85 \\\midrule
  6x6 & 36 &   2.69 &   2.62 &   2.65 &   2.76 &   2.81 &   3.08 &   2.99 &   3.67 \\\midrule
  7x7 & 49 &   2.22 &   2.07 &   2.13 &   2.17 &   2.20 &   2.39 &   2.30 &   2.70 \\\midrule
  8x8 & 64 &   1.92 &   1.72 &   1.73 &   1.83 &   1.83 &   2.05 &   1.88 &   2.32 \\\midrule
  9x9 & 81 &   1.62 &   1.46 &   1.41 &   1.51 &   1.53 &   1.78 &   1.56 &   1.89 \\\bottomrule
\end{tabular}
}
\caption{Strong scaling study data}
\label{table:strong}
\end{table}

\begin{figure}
\centering
\includegraphics[width=0.75\textwidth]{strong.png}
\caption{Strong scaling study performance}
\label{fig:strong}
\end{figure}

The data collected from the strong scaling test is shown in
Table \ref{table:strong}. The data in this table is the wall clock time
required for the computation. We see the processing time decrease as the
number of processors increases. We have also presented this data in
graphical form in Figure \ref{fig:strong}.

\begin{table}
\resizebox{\textwidth}{!}{
\begin{tabular}{cccccccccc}
  \toprule
  sub-domain grid & nproc & \multicolumn{8}{c}{simulation ticks per communication} \\\cmidrule(r){3-10}
                  &                 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\\midrule
  1x1 &  1 & 5.33e4 & 3.47e4 & 3.57e4 & 4.87e4 & 3.88e4 & 4.19e4 & 2.99e4 & 2.59e4 \\\midrule
  2x2 &  4 & 6.22e4 & 4.04e4 & 3.93e4 & 4.20e4 & 3.59e4 & 3.53e4 & 2.92e4 & 6.26e4 \\\midrule
  3x3 &  9 & 3.96e4 & 4.12e4 & 3.72e4 & 3.64e4 & 3.67e4 & 3.55e4 & 3.27e4 & 3.88e4 \\\midrule
  4x4 & 16 & 5.45e4 & 5.21e4 & 5.04e4 & 4.62e4 & 4.92e4 & 4.84e4 & 4.04e4 & 4.37e4 \\\midrule
  5x5 & 25 & 6.79e4 & 6.44e4 & 6.10e4 & 5.89e4 & 5.48e4 & 5.51e4 & 5.00e4 & 5.15e4 \\\midrule
  6x6 & 36 & 8.03e4 & 7.62e4 & 7.32e4 & 7.13e4 & 6.82e4 & 6.23e4 & 5.98e4 & 5.85e4 \\\midrule
  7x7 & 49 & 9.38e4 & 8.82e4 & 8.49e4 & 8.17e4 & 7.67e4 & 7.35e4 & 6.97e4 & 6.67e4 \\\midrule
  8x8 & 64 & 1.02e5 & 9.59e4 & 9.38e4 & 8.99e4 & 8.63e4 & 8.32e4 & 7.82e4 & 7.45e4 \\\midrule
  9x9 & 81 & 1.16e5 & 1.11e5 & 1.05e5 & 9.98e4 & 9.69e4 & 9.43e4 & 8.77e4 & 8.39e4 \\\bottomrule
\end{tabular}
}
\caption{Weak scaling study data}
\label{table:weak}
\end{table}

\begin{figure}
\centering
\includegraphics[width=0.75\textwidth]{weak.png}
\caption{Weak scaling study performance}
\label{fig:weak}
\end{figure}

For the weak scaling study, we collected data on the amount of work
accomplished per time. To get this number, we multiplied the number of
cells in the simulation by the number of time steps computed, and divided
by the wall clock time. This data is presented in Table \ref{table:weak}.
As expected, the size of the problem we can compute in a given amount of
time increases as the number of processors increase. We present a graph of
this data in Figre \ref{fig:weak}.


\section{Conclusion}
What we would add for next time:
\begin{enumerate}
	\item More careful cache performance
	\item Tuning number of ghost cells to block size
	\item Different sized domains/initial conditions
	\item Deal with the fact that not all processors are the same
	\item Think about the tradeoff between a conservative time step and communicating every step (how uneven are the time steps really?)
	\item Communication between processors at different nodes is different than that of the same nodes?
\end{enumerate}


\end{document}
